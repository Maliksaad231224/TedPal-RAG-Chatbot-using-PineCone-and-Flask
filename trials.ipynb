{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpFIHrOrRSMc",
        "outputId": "6030a5e3-e9a9-4f59-e3a7-5b8ddec6dd03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'e:\\\\Bappy\\\\Coding\\\\Youtube\\\\Generative AI\\\\Langchain-Projects\\\\End-to-end-Medical-Chatbot-Generative-AI\\\\research'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE0BJ8STTEAI"
      },
      "source": [
        "# Loading the current directory to Extract the PDF File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqmvv9KkRSMc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"../\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRv_qdhGRSMc",
        "outputId": "bc2e1c84-35fd-4b40-e607-8120c949eed1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'e:\\\\Bappy\\\\Coding\\\\Youtube\\\\Generative AI\\\\Langchain-Projects\\\\End-to-end-Medical-Chatbot-Generative-AI'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usa2FWA3TKQX"
      },
      "source": [
        "# Loading the Document Splitter, Loader Classes from Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWFfBFtWRSMc"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz_cZswfTP4X"
      },
      "source": [
        "# Defining a Custom Load PDF function to Ingest the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT0eYCY9RSMc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_pdf_file(data):\n",
        "    loader= DirectoryLoader(data,\n",
        "                            glob=\"*.pdf\",\n",
        "                            loader_cls=PyPDFLoader)\n",
        "\n",
        "    documents=loader.load()\n",
        "\n",
        "    return documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk-tQX9gTYlX"
      },
      "source": [
        "# Calling the Load PDF Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VE_4zicRSMc"
      },
      "outputs": [],
      "source": [
        "extracted_data=load_pdf_file(data='Data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-EZOCfmTdrn"
      },
      "source": [
        "# Text Splitting\n",
        "## Defing Chunk Size as 500 and Overlap Size as 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7Gq8BEDRSMd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def text_split(extracted_data):\n",
        "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "    text_chunks=text_splitter.split_documents(extracted_data)\n",
        "    return text_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du_VUSAKRSMd",
        "outputId": "1d055773-bdf7-48fc-f3c1-7e77db7f2c86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of Text Chunks 7020\n"
          ]
        }
      ],
      "source": [
        "text_chunks=text_split(extracted_data)\n",
        "print(\"Length of Text Chunks\", len(text_chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7g2EMctToke"
      },
      "source": [
        "# Downloading an Embedding Model from HuggingFcae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIbqp528RSMd",
        "outputId": "b66d9311-caf4-4a9a-ac7a-de9a535b7976"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Anaconda3\\envs\\medibot\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6kArqj5XJDV"
      },
      "source": [
        "# We will use sentemce transformers as our embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPU80QxgRSMd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def download_hugging_face_embeddings():\n",
        "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXtjnsEfTvjG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GupFgJv4RSMd",
        "outputId": "edd4be51-fecd-48c4-ce4c-f52c2926d932"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Bappy Ahmed\\AppData\\Local\\Temp\\ipykernel_21168\\1196424635.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
            "c:\\Anaconda3\\envs\\medibot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Anaconda3\\envs\\medibot\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embeddings = download_hugging_face_embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLg9W691RSMe",
        "outputId": "6dcbbd58-38a3-4194-fc22-e5d3c1ff27f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length 384\n"
          ]
        }
      ],
      "source": [
        "query_result = embeddings.embed_query(\"Hello world\")\n",
        "print(\"Length\", len(query_result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF4AkPSsRSMe",
        "outputId": "ceafaa90-a123-4ecf-96cc-8852e3ea0b39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SCM4TH-V6Tv"
      },
      "outputs": [],
      "source": [
        "\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
        "COHERE_API_KEY = os.getenv('COHERE_KEY')\n",
        "GROQ_API_KEY = os.getenv('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSB9bBppRSMe"
      },
      "outputs": [],
      "source": [
        "from pinecone.grpc import PineconeGRPC as Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "import os\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# this is the database name\n",
        "index_name = \"tedtalks\"\n",
        "\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name, # database name\n",
        "    dimension=384,\n",
        "    metric=\"cosine\" , # we define the similarity metric in this case it is cosine similarity\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\", #  we host pinecone on AWS\n",
        "        region=\"us-east-1\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am0qnUE-WrQF"
      },
      "source": [
        "We set up the environment variables they are API keys which are initilized in your working directory. ALL you have to do is make '.env' file. It has no other name. You have to define the api in the format as below\n",
        "\n",
        "PINECONE_API = 'Write your API'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01-f9ek_RSMe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgmV5ciIXqB7"
      },
      "source": [
        "We load the text chunk and embeddings to th e pinecone database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoI6xrVPRSMe"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "docsearch = PineconeVectorStore.from_documents(\n",
        "    documents=text_chunks,\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR1O_puqUwl1"
      },
      "source": [
        "# Loading all the documents from the Vector Store\n",
        "Now we import the embeddings from the remote vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pERsAH-DRSMe"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "docsearch = PineconeVectorStore.from_existing_index(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "medibot",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
